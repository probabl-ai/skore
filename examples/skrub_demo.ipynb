{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import Bunch\n",
    "from skrub import TableReport, tabular_learner\n",
    "from skrub.datasets import fetch_employee_salaries\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skore import load\n",
    "from skore.item import MediaItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_MANDER = \"datamander\"\n",
    "PATH_PROJECT = Path(\"skrub_demo\")\n",
    "N_SEEDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a project at path './skrub_demo.skore'\n",
    "!python -m skore create skrub_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Launch the web UI with `python -m skore launch skrub_demo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_ridge():\n",
    "    return tabular_learner(RidgeCV())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rf():\n",
    "    return tabular_learner(RandomForestRegressor(n_jobs=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_gb():\n",
    "    return tabular_learner(HistGradientBoostingRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_MODEL_FUNC = {\n",
    "    \"ridge\": init_ridge,\n",
    "    \"rf\": init_rf,\n",
    "    \"gb\": init_gb,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(model_names):\n",
    "    results = []\n",
    "    for model_name in model_names:\n",
    "        print(f\"{' Evaluating ' + model_name + ' ':=^50}\")\n",
    "        results.append(evaluate_seeds(model_name))\n",
    "\n",
    "    project = load(PATH_PROJECT)\n",
    "    project.put_item(\n",
    "        \"skrub_report\",\n",
    "        MediaItem.factory(plot_skrub_report(), media_type=\"text/html\"),\n",
    "    )\n",
    "\n",
    "    project.put(\"target_distribution\", plot_y_distribution())\n",
    "    project.put(\"Metrics\", plot_table_metrics(results))\n",
    "    project.put(\"R2 vs fit time\", plot_r2_vs_fit_time(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_seeds(model_name):\n",
    "    path_model = PATH_PROJECT / model_name\n",
    "\n",
    "    seed_scores = []\n",
    "    for random_state in tqdm(range(N_SEEDS)):\n",
    "        bunch = get_data(random_state)\n",
    "        model = INIT_MODEL_FUNC[model_name]()\n",
    "\n",
    "        tic = time()\n",
    "        model.fit(bunch.X_train, bunch.y_train)\n",
    "        fit_time = time() - tic\n",
    "\n",
    "        scores = evaluate(model, bunch)\n",
    "        scores.update(\n",
    "            {\n",
    "                \"random_state\": random_state,\n",
    "                \"model_name\": model_name,\n",
    "                \"fit_time\": fit_time,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        path_seed = path_model / f\"random_state{random_state}\"\n",
    "\n",
    "        project = load(PATH_PROJECT)\n",
    "        project.put(path_seed / \"scores\", scores)  # scores is a dict\n",
    "        project.put_item(\n",
    "            path_seed / \"model_repr\",\n",
    "            MediaItem.factory(plot_model_repr(model), media_type=\"text/html\"),\n",
    "        )\n",
    "        project.put(\n",
    "            path_seed / \"feature importance\", plot_feature_importance(model, bunch)\n",
    "        )\n",
    "        seed_scores.append(scores)\n",
    "\n",
    "    agg_scores = aggregate_seeds_results(seed_scores)\n",
    "    project.put(path_model / \"agg_scores\", agg_scores)\n",
    "\n",
    "    return agg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, bunch):\n",
    "    y_pred = model.predict(bunch.X_test)\n",
    "    y_test = bunch[\"y_test\"]\n",
    "\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    scores = {\n",
    "        \"y_pred\": y_pred.tolist(),\n",
    "        \"r2\": r2,\n",
    "        \"mae\": mae,\n",
    "        \"mse\": mse,\n",
    "    }\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_seeds_results(scores):\n",
    "    agg_score = dict()\n",
    "    for metric in [\"r2\", \"mae\", \"mse\", \"fit_time\"]:\n",
    "        score_seeds = [score[metric] for score in scores]\n",
    "        agg_score.update(\n",
    "            {\n",
    "                f\"mean_{metric}\": np.mean(score_seeds),\n",
    "                f\"std_{metric}\": np.std(score_seeds),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    agg_score[\"model_name\"] = scores[0][\"model_name\"]\n",
    "\n",
    "    return agg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(random_state, split=True):\n",
    "    dataset = fetch_employee_salaries()\n",
    "    X, y = dataset.X, dataset.y\n",
    "    if split:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, random_state=random_state\n",
    "        )\n",
    "        return Bunch(\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "        )\n",
    "    else:\n",
    "        return Bunch(X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_table_metrics(results):\n",
    "    df = pd.DataFrame(results)\n",
    "    rename = {\n",
    "        \"r2\": \"R2 (↑)\",\n",
    "        \"mse\": \"MSE (↓)\",\n",
    "        \"mae\": \"MAE (↓)\",\n",
    "        \"fit_time\": \"Fit time (↓)\",\n",
    "    }\n",
    "\n",
    "    for metric in [\"r2\", \"mae\", \"mse\", \"fit_time\"]:\n",
    "        mean_key, std_key = f\"mean_{metric}\", f\"std_{metric}\"\n",
    "        df[rename[metric]] = (\n",
    "            df[mean_key].round(4).astype(str) + \" ± \" + df[std_key].round(4).astype(str)\n",
    "        )\n",
    "        df = df.drop([mean_key, std_key], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.figure\n",
    "\n",
    "\n",
    "def plot_r2_vs_fit_time(results) -> matplotlib.figure.Figure:\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    model_names = df[\"model_name\"].tolist()\n",
    "    palette = dict(\n",
    "        zip(\n",
    "            list(model_names),\n",
    "            sns.color_palette(\"colorblind\", n_colors=len(model_names)),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5), dpi=100)\n",
    "    c = \"black\"\n",
    "    plt.errorbar(\n",
    "        x=df[\"mean_fit_time\"],\n",
    "        y=df[\"mean_r2\"],\n",
    "        yerr=df[\"std_r2\"],\n",
    "        fmt=\"none\",\n",
    "        c=c,\n",
    "        capsize=2,\n",
    "    )\n",
    "    plt.errorbar(\n",
    "        x=df[\"mean_fit_time\"],\n",
    "        xerr=df[\"std_fit_time\"],\n",
    "        y=df[\"mean_r2\"],\n",
    "        fmt=\"none\",\n",
    "        c=c,\n",
    "        capsize=2,\n",
    "    )\n",
    "    ax = sns.scatterplot(\n",
    "        df,\n",
    "        x=\"mean_fit_time\",\n",
    "        y=\"mean_r2\",\n",
    "        hue=\"model_name\",\n",
    "        s=200,\n",
    "        palette=palette,\n",
    "        zorder=10,\n",
    "        alpha=1,\n",
    "    )\n",
    "\n",
    "    ax.grid()\n",
    "    sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "    # plt.tight_layout()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_skrub_report():\n",
    "    bunch = get_data(random_state=0, split=False)\n",
    "    df = pd.concat([bunch.X, bunch.y], axis=1)\n",
    "    return TableReport(df).html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, bunch) -> alt.Chart:\n",
    "    importances = permutation_importance(model, bunch.X_test, bunch.y_test, n_jobs=4)\n",
    "\n",
    "    feature_imp = pd.DataFrame(\n",
    "        importances[\"importances\"].T, columns=bunch.X_train.columns\n",
    "    ).melt()  # Convert the dataframe to a long format\n",
    "\n",
    "    return (\n",
    "        alt.Chart(feature_imp)\n",
    "        .mark_boxplot(extent=\"min-max\")\n",
    "        .encode(\n",
    "            alt.X(\"value:Q\").scale(domain=[0, 1]),\n",
    "            alt.Y(\"variable:N\"),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_y_distribution() -> alt.Chart:\n",
    "    bunch = get_data(random_state=0, split=False)\n",
    "    df = pd.concat([bunch.X, bunch.y], axis=1)\n",
    "    N = min(1000, df.shape[0])\n",
    "    df = df.sample(N)\n",
    "\n",
    "    # alt.data_transformers.enable(\"vegafusion\")\n",
    "\n",
    "    return (\n",
    "        alt.Chart(df)\n",
    "        .mark_bar()\n",
    "        .encode(\n",
    "            x=alt.X(\"current_annual_salary:Q\", bin=alt.Bin(maxbins=30)),\n",
    "            y=\"count()\",\n",
    "            color=\"gender:N\",\n",
    "        )\n",
    "        .properties(width=600, height=400)\n",
    "        .interactive()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_repr(model) -> str:\n",
    "    return model._repr_html_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    evaluate_models(model_names=list(INIT_MODEL_FUNC))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
